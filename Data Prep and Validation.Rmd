---
title: "Data Preparation and Clustering Validation"
output: github_document
---

# Preparation of dataset and Clustering Validation

```{r}
#Load the packages
load(".RData")
packages <- readLines("requirements.txt")
lapply(packages, library, character.only = TRUE)
```

## Behavioural data

As a starting point of the customer segmentation, I will pick out only the behavioural attributes from the 'raw' dataframe. This new dataframe will be called behavioural_data and all the columns must be scaled. Considerint, that there are only numeric values in all columns, I need to transform them with a scaling function. The data will be fit within a specific range/distribution to improve the performance of the model.
```{r}
behavioral_data<-raw %>% data.frame() %>% 
  select(InternetTrafficVolume,
         MortageVolume,
         AccountSpending,
         CreditCardSpending,
         HelpHotlineTime,
         CustomerSince,
         GrocerySpending,
         StockVolume,
         CreditVolume,
         NASDAQInvest,
         USAXSFundInvest,
         BranchVisits,
         AppLogins,
         ATMVisits,
         TimeOnlineBanking,
         ServiceFees,
         SocialMediaInter,
         Bitcoins,
         NFTs)
head(behavioral_data)
```
### Data Scaling

```{r}
set.seed(123) #Insert the same seed to get the same scaled numerical values
behavioral_data_scaled<-scale(behavioral_data)
head(behavioral_data_scaled)
```
### Distance between objects

```{r}
random_subset<-behavioral_data_scaled[sample(nrow(behavioral_data_scaled),1000), ]
dist.eucl<-get_dist(random_subset, method = "euclid")
round(as.matrix(dist.eucl)[1:3,1:3],1)
fviz_dist(dist.eucl)
hist(dist.eucl)
median(dist.eucl)
```
I took a random sample of scaled data, because calculating distance between objects is heavy process when running locally.

I used the Euclidean distance method to measure and overview the distribution of a random sample of objects. High values mean high separation between clusters and low value could mean tight cohesion inside clusters. The distribution of the colours on the matrix shows that there are underlying clusters.In order to fully understand those requirements of clusterisation, I will provide additional metrics and scores.

## Validation of cluster formation + optimal k number of clusters

This validation is prerequisite before modelling, so we have a basic idea of what the data looks like when plotted. In simple terms, I will be looking to find 'clouds' on the following chart. If there are obvious chunks of data points, that means we have good probability to build good clustering model.

### Visual inspection
```{r}
fviz_pca_ind(prcomp(behavioral_data_scaled),
             geom = "point", palette = 'jco',
             ggtheme = theme_classic(),
             title = 'Visual inspection of the data for clusterization')
```
It becomes clear there are 6 visible groups of data. However, that does not mean that 6 clusters is the optimal number we want to use for the modelling. There are other methods, which will provide better orientation.

### Hopkins' Test for evenly statistically distributed data
This is a statistical hypothesis test, which defines a Null Hypothesis that the data is generated by a Poisson process, consequently the data is evenly distributed. If the value of the Hopkins test is above 0.7 to 1, that means the data has good tendency to be clustered or we can also say that the data is not random and there is connection between segments.
```{r}
set.seed(123)
hopkins_test<-hopkins(behavioral_data_scaled,m = nrow(behavioral_data_scaled)-1)
print(paste0("Hopkins statistic: ", hopkins_test))

h_pval<-hopkins.pval(hopkins_test, n = nrow(behavioral_data_scaled)-1)
print(paste0("p-value for statistical significance: ", h_pval))
```
The data show a high tendency for clustering hopkins_test = 0.99, and Hopkins statistical significance below 0.05, i.e. it is significant.

### Finding an optimal number of clusters

In this part of the project I will apply 4 different methods when choosing the optimal number of clusters. The main function we use is fviz_nbclust from the factoextra package. The most advanced method that we will use is NbClust() from the NbClust package, which provides the widest scope of clustering validation results. For all of the methods, I used kmeans as the algorithm to compare with.

- Elbow method - estimates the compactness of the clusterisation and we want it as low as possible
- Average Silhoute method - estimates the homogeneity of every data point and how well it fits into its own cluster. We look for the max value.
- Gap statistics - iteratively compares the variance of clusters based on different k number of clusters. We aim to choose the highest value for gap statistics and its respective k-number

```{r}
# Elbow method
wss<-fviz_nbclust(behavioral_data_scaled, kmeans, method = "wss") + labs(title = "Optimal number of clusters (Elbow method)")
wss

# Silhouette method
silh<-fviz_nbclust(behavioral_data_scaled, kmeans, method = "silhouette") +labs(title = "Optimal number of clusters (Silhouette method)")
silh

# Gap statistics
set.seed(123)
pca_sample<- prcomp(behavioral_data_scaled)
reduced_data<-pca_sample$x[ ,1:3]
gap<-fviz_nbclust(reduced_data, FUNcluster = function(x,k) kmeans(x, centers = k, nstart = 10, iter.max = 100), 
    method = "gap_stat", 
    nboot = 20) + labs(title = "Optimal number of clusters (Gap statistics)")
gap
```
NbClust is the most advanced method, which allows me to compare withing a range of k (2-8).

!Please note that NbClust is heavy function to process locally on standard PC and it usually takes time!
```{r}
nb<-NbClust(data = behavioral_data_scaled, distance = "euclidean", min.nc = 2, max.nc = 8, method = "kmeans") #Pick min and max number of clusters.
barNbclust<-barplot(table(nb$Best.nc[1,]), xlab ="Number of clusters", ylab = "Number of criterias")
barNbclust
```
The optimal number of clusters after running NbClust is 7. Also, with 4 clusters, we can read a good evaluation of the validation method.

The conclusion we can draw after validating the number of clusters is that they vary between 6 and 8. Only the NbClust function shows a good estimate for 4 segments. It is significant to note that both Elbow and Silhouette method have a smoothing in the line at around 5 clusters.

## Next: Clustering analysis and choosing the best model
In the next part of the project, I will test out the majority of clustering models and algorithms. The test will be performed with k (number of clusters) around 7. Right below every model, I will analyse the model with validating visualizations and evaluations.