---
title: "Cluster Analysis"
output:
  html_document:
    df_print: paged
---

# Choosing the right ML model for a clusterization of customer segments
```{r}
packages <- readLines("requirements.txt")
lapply(packages, library, character.only = TRUE)
```


When conducting each of the different models, a graph will be presented to visualize the clustering. There will also be a visualization showing the average silhouette width, which is to validate the homogeneity of each cluster (a good value is close to 1). Finally, the following indices will be presented to validate each algorithm internally:

- Dunn's index - Above 0.3, we have good cluster separation
- Average silhouette width - Above 0.5 means good clustering
- Average distance within clusters - As low as possible (homogeneity)
- Average distance between clusters - As large as possible (heterogeneity)
- Pearson-Gamma - Up to 1 means a strong relationship between distance and clustering

After a detailed analysis of the clustering methods studied, an optimal decision will be made.

## KMEANS

I will use the eclust function from the factoextra package because it creates a silhouette object that is used to validate the result.

Algorithms like KMEANS, PAM, CLARA all work by defining a definite number for 'k', which is the number of clusters that we algorithm will use as base for the grouping of all data points.In the previous part of project 'Data Prep', I already analysed the optimal approximate number ofclusters. The range is between 6 - 8 clusters.
### 7 Clusters
```{r}
km_res1 <- eclust(behavioral_data_scaled, "kmeans", k=7, nstart=50, iter.max=100, graph = FALSE)

fviz_cluster(km_res1,
             data=behavioral_data_scaled,
             ellipse.type = "euclid",
             repel = FALSE,
             ggtheme = theme_classic(),
             palette = 'jco',
             main = "K-means with 7 CLUSTERS")

fviz_silhouette(km_res1, palette = 'jco',
                ggtheme = theme_classic(),
                print.summary = FALSE)

km1_stats<-fpc::cluster.stats(d=dist(behavioral_data_scaled), clustering = km_res1$cluster)
print(paste0("Dunn Index: ", km1_stats$dunn))
print(paste0("Average Silhouette Width: ", km1_stats$avg.silwidth))
print(paste0("Average Within-Cluster Distance: ", km1_stats$average.within))
print(paste0("Average Between-Cluster Distance: ", km1_stats$average.between))
print(paste0("Strength of Association Between Distance and Clustering: ", km1_stats$pearsongamma))
```

### 6 Clusters
```{r}
km_res2 <- eclust(behavioral_data_scaled, "kmeans", k=6, nstart=50, iter.max=100, graph = FALSE)

fviz_cluster(km_res2,
             data=behavioral_data_scaled,
             ellipse.type = "euclid",
             repel = FALSE,
             ggtheme = theme_classic(),
             palette = 'jco',
             main = "K-means with 6 CLUSTERS")

fviz_silhouette(km_res2, palette = 'jco',
                ggtheme = theme_classic(),
                print.summary = FALSE)

km2_stats<-fpc::cluster.stats(d=dist(behavioral_data_scaled), clustering = km_res2$cluster)
print(paste0("Dunn Index: ", km2_stats$dunn))
print(paste0("Average Silhouette Width: ", km2_stats$avg.silwidth))
print(paste0("Average Within-Cluster Distance: ", km2_stats$average.within))
print(paste0("Average Between-Cluster Distance: ", km2_stats$average.between))
print(paste0("Strength of Association Between Distance and Clustering: ", km2_stats$pearsongamma))
```

When performing cluster analysis with the k-means algorithm with 7 clusters, we obtain the highest internal validation coefficient scores. Average Silhouette width = 0.71 and Dunn = 0.38.

## K-MEDOIDS (PAM)
This is another clustering approach that is a reliable alternative to k-means for data with outliers, as in our dataset. This method (PAM) uses the most centrally located point in a given cluster based on the median rather than the arithmetic mean like k-means.

### 6 Clusters
```{r}
pam_res1<-eclust(behavioral_data_scaled, "pam", k=6, graph = FALSE)

fviz_cluster(pam_res1,
             data=behavioral_data_scaled,
             ellipse.type = "euclid",
             repel = FALSE,
             ggtheme = theme_classic(),
             palette = 'jco',
             main = "PAM 6 CLUSTERS")

fviz_silhouette(pam_res1, palette = 'jco',
                ggtheme = theme_classic(),
                print.summary = FALSE)

pam1_stats<-fpc::cluster.stats(d=dist(behavioral_data_scaled), clustering = pam_res1$clustering)
print(paste0("Dunn Index: ", pam1_stats$dunn))
print(paste0("Average Silhouette Width: ", pam1_stats$avg.silwidth))
print(paste0("Average Within-Cluster Distance: ", pam1_stats$average.within))
print(paste0("Average Between-Cluster Distance: ", pam1_stats$average.between))
print(paste0("Strength of Association Between Distance and Clustering: ", pam1_stats$pearsongamma))
```

### 7 Clusters
```{r}
pam_res2<-eclust(behavioral_data_scaled, "pam", k=7, graph = FALSE)

fviz_cluster(pam_res2,
             data=behavioral_data_scaled,
             ellipse.type = "euclid",
             repel = FALSE,
             ggtheme = theme_classic(),
             palette = 'jco',
             main = "PAM 7 CLUSTERS")

fviz_silhouette(pam_res2, palette = 'jco',
                ggtheme = theme_classic(),
                print.summary = FALSE)

pam2_stats<-fpc::cluster.stats(d=dist(behavioral_data_scaled), clustering = pam_res2$clustering)
print(paste0("Dunn Index: ", pam2_stats$dunn))
print(paste0("Average Silhouette Width: ", pam2_stats$avg.silwidth))
print(paste0("Average Within-Cluster Distance: ", pam2_stats$average.within))
print(paste0("Average Between-Cluster Distance: ", pam2_stats$average.between))
print(paste0("Strength of Association Between Distance and Clustering: ", pam2_stats$pearsongamma))
```

When performing PAM with 7 clusters, there is also better clustering of segment number 2 on the first attempt. These are the groups in the upper right corner. Since PAM with 6 clusters identified 3 segments in the middle of the graph, these 2 in the upper right corner fell below the overall median, but some of the objects are quite distant and may not need to belong to a single cluster.

## CLARA

CLARA works in the same way as PAM, but is a suitable model for large data sets. It divides the set into samples and calculates PAM for each one until it finds the optimal sample and clustering that best describes all the data. The set with the minimum sum of dissimilarities is retained.

### 6 Clusters

```{r}
clara_res1<-clara(behavioral_data_scaled,k=6, metric = "euclidean", stand = FALSE, samples = 1000, pamLike = TRUE)

fviz_cluster(clara_res1,
             data=behavioral_data_scaled,
             ellipse.type = "euclid",
             repel = FALSE,
             ggtheme = theme_classic(),
             palette = 'jco',
             main = "CLARA 6 CLUSTERS")

fviz_silhouette(clara_res1, palette = 'jco',
                ggtheme = theme_classic(),
                print.summary = FALSE)

clara1_stats<-fpc::cluster.stats(d=dist(behavioral_data_scaled), clustering = clara_res1$clustering)
print(paste0("Коефициент на Дюн: ", clara1_stats$dunn))
print(paste0("Средна ширина на силуета: ", clara1_stats$avg.silwidth))
print(paste0("Средно разстояние в рамките на клъстерите: ", clara1_stats$average.within))
print(paste0("Средно разстояние между клъстерите: ", clara1_stats$average.between))
print(paste0("Сила на връзката между разстояние и клъстеризация: ", clara1_stats$pearsongamma))
```

### 7 Clusters
```{r}
clara_res2<-clara(behavioral_data_scaled,k=7, metric = "euclidean", stand = FALSE, samples = 1000, pamLike = TRUE)

fviz_cluster(clara_res2,
             data=behavioral_data_scaled,
             ellipse.type = "euclid",
             repel = FALSE,
             ggtheme = theme_classic(),
             palette = 'jco',
             main = "CLARA 7 CLUSTERS")

fviz_silhouette(clara_res2, palette = 'jco',
                ggtheme = theme_classic(),
                print.summary = FALSE)

clara2_stats<-fpc::cluster.stats(d=dist(behavioral_data_scaled), clustering = clara_res2$clustering)
print(paste0("Коефициент на Дюн: ", clara2_stats$dunn))
print(paste0("Средна ширина на силуета: ", clara2_stats$avg.silwidth))
print(paste0("Средно разстояние в рамките на клъстерите: ", clara2_stats$average.within))
print(paste0("Средно разстояние между клъстерите: ", clara2_stats$average.between))
print(paste0("Сила на връзката между разстояние и клъстеризация: ", clara2_stats$pearsongamma))
```

After running CLARA with 6 and 7 clusters, we get higher validation scores with 7 clusters. I used 1000 samples in both experiments, and the 7-cluster model has the highest average silhouette width score (0.76) so far. **The algorithm runs relatively faster than PAM.**

## Hierarchical KMEANS
### 7 Clusters
The hierarchical method of clustering is an old well-known model. Each data object is initially reviewed as a single cluster, then the most similar objects are clustered into bigger segments. This model can be easily visualized like a decision tree. There is also a divisive method of the Hierarchical model, which is the opposite. So the starting point is one big cluster, which gets divided by the heterogeneity of the objects. However, both methods come with very high computational cost, that is why the Hybrid model of Hierarchical KMEANS is a very efficient way of splitting the data into manageable partitions, that way we can use this model one bigger data sets. In addition, we get more balanced tree structures in comparison to the ordinary Hierarchical clustering, flexible granularity of the levels and handling of underlying nested structures.
```{r}
hc_distance<-dist(x=behavioral_data_scaled, method = "euclidean")

hkm_res1 <- hkmeans(behavioral_data_scaled, k = 7, hc.metric = "euclidean", hc.method = "average", iter.max = 50)

fviz_cluster(hkm_res1,
             data=behavioral_data_scaled,
             ellipse.type = "euclid",
             repel = FALSE,
             ggtheme = theme_classic(),
             palette = 'jco',
             main = "HIERARCHICAL KMEANS 7 CLUSTERS")

hk_sil<-silhouette(x=hkm_res1$cluster,
                   dist = hc_distance)
fviz_silhouette(hk_sil, palette = 'jco',
                ggtheme = theme_classic(),
                print.summary = FALSE)

hkm_res1_stats<-fpc::cluster.stats(d=dist(behavioral_data_scaled), clustering = hkm_res1$cluster)
print(paste0("Dunn index: ", hkm_res1_stats$dunn))
print(paste0("Average silhouette width: ", hkm_res1_stats$avg.silwidth))
print(paste0("Average within-cluster distance: ", hkm_res1_stats$average.within))
print(paste0("Average between-cluster distance: ", hkm_res1_stats$average.between))
print(paste0("Strength of association between distance and clustering: ", hkm_res1_stats$pearsongamma))
```
I tested the model with 6 and 7 clusters. They were pretty similar, but 7 clusters led to higher average silhouette width.

## Clusterisation based on probabillity models
## Gaussian Mixture Models (GMM)

There may be several different Gaussian distributions in the data set. Objects are grouped according to their membership in a given distribution. Each Gaussian distribution is treated as a separate cluster. When updating the centroid, the mean value and variance are used, which makes the model better than kmeans.
```{r}
fmm_res<-Mclust(behavioral_data_scaled, G = 2:9) # The model automatically chooses the number of clusters
```
```{r}
cat(paste0("Model type: ", fmm_res$modelName, "\nNumber of clusters: ",fmm_res$G))
```
**The model finds the optimal solution with 7 clusters with an ellipsoidal shape.**
```{r}
fviz_mclust(fmm_res,
            "classification",
            palette="jco", main = "FMM with 7 CLUSTERS")

fmm_sil<-silhouette(x=fmm_res$classification,
                  dist(behavioral_data_scaled, method = "euclidean"))

fviz_silhouette(fmm_sil, palette = 'jco',
                ggtheme = theme_classic(),
                print.summary = FALSE)

fmm_res1_stats<-fpc::cluster.stats(d=dist(behavioral_data_scaled), clustering = fmm_res$classification)
print(paste0("Dunn index: ", fmm_res1_stats$dunn))
print(paste0("Average silhouette width: ", fmm_res1_stats$avg.silwidth))
print(paste0("Average within-cluster distance: ", fmm_res1_stats$average.within))
print(paste0("Average between-cluster distance: ", fmm_res1_stats$average.between))
print(paste0("Strength of association between distance and clustering: ", fmm_res1_stats$pearsongamma))

```

## DBSCAN
Clustering based on density. This model is particularly good at finding groups with high density and arbitrary shape in space, which helps to easily identify and exclude anomalies such as those we have in the dataset.

I chose the minPts parameter to be at least twice as large as the number of behavioral variables we are examining for customers. With a lower value of this parameter and epsilon, there are quite a few incorrectly clustered objects in cluster number 7. I noticed this on the average silhouette width graph. The selected value for eps was obtained after determining the radius with a function for K number of neighbors (kNNdistplot).


```{r}
set.seed(123)
dbscan_res<-fpc::dbscan(behavioral_data_scaled, eps=1.5, MinPts = 200)

fviz_cluster(dbscan_res,
             data=behavioral_data_scaled,
             palette = "jco",
             repel = FALSE,
             ggtheme = theme_classic(),
             ellipse.type = "convex",
             main = "DBSCAN with 7 CLUSTERS")

db_sil<-silhouette(dbscan_res$cluster, dist = hc_distance)
fviz_silhouette(db_sil, palette = 'jco',
                ggtheme = theme_classic(),
                print.summary = FALSE)

dbscan_stats<-fpc::cluster.stats(d=dist(behavioral_data_scaled), clustering = dbscan_res$cluster)
print(paste0("Dunn index: ", dbscan_stats$dunn))
print(paste0("Average silhouette width: ", dbscan_stats$avg.silwidth))
print(paste0("Average within-cluster distance: ", dbscan_stats$average.within))
print(paste0("Average between-cluster distance: ", dbscan_stats$average.between))
print(paste0("Strength of association between distance and clustering: ", dbscan_stats$pearsongamma))
```

**Number of clusters after running DBSCAN**

```{r}
length(unique(dbscan_res$cluster)) - 1
# I subtract 1, because outliers are counted as another cluster
```
**Count of objects, which are outliers**
```{r}
sum(dbscan_res$cluster == 0)
```
**The graph of the average silhouette width shows cluster 0, which identifies all anomalies as a separate cluster. The optimal number of clusters found by DBSCAN is 7.**

Determining the radius of proximity
```{r}
dbscan::kNNdistplot(behavioral_data_scaled, k=200)
```
So, the optimal value for Epsilon is **1.5**

## Final comparison between all models
```{r}
# Example: manually create a table with model names, variable names, and k
results <- data.frame(
  Model    = c("K-MEANS", "K-MEANS", "PAM", "PAM", "CLARA", "CLARA", "Hierarchical K-MEANS", "GMM", "DBSCAN"),
  Var.Name = c("km_res1", "km_res2", "pam_res1", "pam_res2", "clara_res1", "clara_res2", "hkm_res1", "fmm_res", "dbscan_res"),
  k        = c(7, 6, 6, 7, 6, 7, 7, 7, 5) # just example k values
)

# Add computed metrics from your stored stats objects
results$Dunn <- c(
  km1_stats$dunn,
  km2_stats$dunn,
  pam1_stats$dunn,
  pam2_stats$dunn,
  clara1_stats$dunn,
  clara2_stats$dunn,
  hkm_res1_stats$dunn,
  fmm_res1_stats$dunn,
  dbscan_stats$dunn
)

results$Avg.Sil.Width <- c(
  km1_stats$avg.silwidth,
  km2_stats$avg.silwidth,
  pam1_stats$avg.silwidth,
  pam2_stats$avg.silwidth,
  clara1_stats$avg.silwidth,
  clara2_stats$avg.silwidth,
  hkm_res1_stats$avg.silwidth,
  fmm_res1_stats$avg.silwidth,
  dbscan_stats$avg.silwidth
)

results$Avg.Within <- c(
  km1_stats$average.within,
  km2_stats$average.within,
  pam1_stats$average.within,
  pam2_stats$average.within,
  clara1_stats$average.within,
  clara2_stats$average.within,
  hkm_res1_stats$average.within,
  fmm_res1_stats$average.within,
  dbscan_stats$average.within
)

results$Average.Between <- c(
  km1_stats$average.between,
  km2_stats$average.between,
  pam1_stats$average.between,
  pam2_stats$average.between,
  clara1_stats$average.between,
  clara2_stats$average.between,
  hkm_res1_stats$average.between,
  fmm_res1_stats$average.between,
  dbscan_stats$average.between
)

results$Pearson.Gamma <- c(
  km1_stats$pearsongamma,
  km2_stats$pearsongamma,
  pam1_stats$pearsongamma,
  pam2_stats$pearsongamma,
  clara1_stats$pearsongamma,
  clara2_stats$pearsongamma,
  hkm_res1_stats$pearsongamma,
  fmm_res1_stats$pearsongamma,
  dbscan_stats$pearsongamma
)

# View table nicely
knitr::kable(results, digits = 2, caption = "Cluster Validation Metrics")
```

### Analysis and final decision for a clustering model

Most of the validation methods for determining the number of clusters suggested 7–8 clusters, which closely aligns with the obtained results. For the clustering methods where I explicitly set the number of clusters, I used 6 or 7, since these configurations produced the most optimal outcomes in terms of internal validation. As shown, the indices and coefficients are quite similar, with the results for 7 clusters being slightly better. The final decision, however, should also take into account the marketing strategy and budget, but the overall analysis can confidently recommend segmenting the customers into 6 or 7 distinct groups.

Even during the initial visualization of the data clusters, a large concentration of objects was noticeable in the center of the plot. For reference, these correspond to clusters 2 and 3 in km_res1. Except in the case of pam_res1, these two clusters are often merged when clustering with k = 6, while running the same model with 7 clusters leads to them being separated. It is likely that these objects differ in one specific attribute but share many others, which causes them to overlap in the 2D projection.

For the purpose of interpreting each cluster, I will choose the 6-cluster solution, as I believe it provides more distinct and consistent segments based on their socio-demographic characteristics.

**My final choice is CLARA with 6 clusters (clara_res1), because it was computationally efficient by sampling from the original dataset. Additionally, the segmentation of each cluster is based on its most central object with respect to the median, which makes this approach more robust in the presence of anomalies.**

## Next: Analysis, Interpretation and Marketing strategy
In the next notebook, I will do an interpretation and analysis of the different customer segments that we have after applying the CLARA Clustering model. In addition, I will present different examples of customer personas and possible marketing strategies.